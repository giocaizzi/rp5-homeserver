# Langfuse - LLM Observability Platform
server {
    listen 443 ssl;
    http2 on;
    server_name langfuse.home;

    include snippets/ssl-params.conf;
    include snippets/error-503.conf;

    # API endpoints - higher rate limit for SDK ingestion
    location /api/public/ {
        # No rate limiting for telemetry ingestion
        set $upstream $backend;
        proxy_pass http://$upstream;
        include snippets/proxy-headers.conf;
        
        # Increase body size for trace batches
        client_max_body_size 10m;
        
        # Timeouts for ingestion
        proxy_connect_timeout 10s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        # Disable buffering for real-time ingestion
        proxy_buffering off;
    }

    location ~* \.(css|js|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
        set $upstream $backend;
        proxy_pass http://$upstream;
        include snippets/proxy-headers.conf;
        include snippets/static-assets.conf;
    }

    location / {
        limit_req zone=general burst=30 nodelay;
        set $upstream $backend;
        proxy_pass http://$upstream;
        include snippets/proxy-headers.conf;
        include snippets/websocket-support.conf;
        
        # Timeouts for dashboard queries
        proxy_connect_timeout 30s;
        proxy_send_timeout 120s;
        proxy_read_timeout 120s;
    }
}
