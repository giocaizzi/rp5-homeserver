# =============================================================================
# YAML Anchors - Common configurations
# =============================================================================
x-labels-base: &labels-base
  com.giocaizzi.namespace: "ollama"
  com.giocaizzi.env: "production"

x-deploy-base: &deploy-base
  mode: replicated
  replicas: 1
  placement:
    constraints:
      - node.role == manager
  restart_policy:
    condition: any
    delay: 5s
    max_attempts: 3
    window: 120s

# =============================================================================
# Networks
# =============================================================================
networks:
  # Private network for ollama stack
  ollama_network:
    driver: overlay
    name: rp5_ollama

  # External network to connect to nginx proxy
  public_network:
    external: true
    name: rp5_public

# =============================================================================
# Services
# =============================================================================
services:
  ollama:
    image: ollama/ollama:latest
    hostname: ollama
    expose:
      - "11434"
    environment:
      # Server configuration
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_MODELS=/root/.ollama
      - OLLAMA_DEBUG=false
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_NUM_PARALLEL=1
      # Raspberry Pi optimizations
      - OLLAMA_MAX_VRAM=1024
      - OLLAMA_FLASH_ATTENTION=false
    volumes:
      - ollama_data:/root/.ollama   # persists models, configs
      - ./ollama-entrypoint.sh:/usr/local/bin/ollama-entrypoint.sh:ro
    networks:
      - ollama_network
      - public_network
    entrypoint: ["/usr/local/bin/ollama-entrypoint.sh"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s  # Allow more time for model loading
    labels:
      <<: *labels-base
      com.giocaizzi.service: "ollama"
      com.giocaizzi.component: "app"
      com.giocaizzi.role: "ai"
      com.giocaizzi.tier: "core"
    security_opt:
      - no-new-privileges:true
      - seccomp:unconfined  # Required for some AI operations
    # GPU access (if available)
    # devices:
    #   - /dev/dri:/dev/dri  # For Intel GPU acceleration
    deploy:
      <<: *deploy-base
    tmpfs:
      - /tmp:size=1G,noexec,nosuid,nodev
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack: 67108864
    logging:
      driver: "json-file"
      options:
        max-size: "50m"  # Larger logs for AI operations
        max-file: "3"

volumes:
  ollama_data:
    driver: local